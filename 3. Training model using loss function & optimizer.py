# Databricks notebook source
# MAGIC %md
# MAGIC ğŸ§­ What You'll Learn
# MAGIC Define a simple dataset
# MAGIC
# MAGIC Use loss function (e.g., MSE for regression)
# MAGIC
# MAGIC Use optimizer (e.g., SGD)
# MAGIC
# MAGIC Train the model (forward â†’ loss â†’ backward â†’ update)
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ğŸ—ï¸ Step 1: Define a Simple Dataset
# MAGIC Letâ€™s say we want to learn this function:
# MAGIC
# MAGIC ğ‘¦ =
# MAGIC 2
# MAGIC ğ‘¥
# MAGIC 1
# MAGIC +
# MAGIC 3
# MAGIC ğ‘¥
# MAGIC 2
# MAGIC y=2x 
# MAGIC 1
# MAGIC â€‹
# MAGIC  +3x 
# MAGIC 2
# MAGIC â€‹
# MAGIC  
# MAGIC Hereâ€™s how we can create some training data:

# COMMAND ----------

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Set random seed for reproducibility
torch.manual_seed(0)

# Inputs: 4 samples with 2 features each
X = torch.tensor([[1.0, 2.0],
                  [2.0, 3.0],
                  [3.0, 4.0],
                  [4.0, 5.0]])

# Outputs using y = 2*x1 + 3*x2
y = torch.tensor([[8.0],
                  [13.0],
                  [18.0],
                  [23.0]])


# COMMAND ----------

# MAGIC %md
# MAGIC ğŸ§  Step 2: Define the Model
# MAGIC
# MAGIC Same as before:

# COMMAND ----------

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(2, 1)  # Directly map 2 inputs to 1 output

    def forward(self, x):
        return self.fc1(x)

model = SimpleNet()


# COMMAND ----------

# MAGIC %md
# MAGIC ğŸ’” Step 3: Define the Loss Function
# MAGIC
# MAGIC Since this is a regression problem:

# COMMAND ----------

criterion = nn.MSELoss()


# COMMAND ----------

# MAGIC %md
# MAGIC âš™ï¸ Step 4: Define the Optimizer
# MAGIC
# MAGIC Weâ€™ll use Stochastic Gradient Descent (SGD):

# COMMAND ----------

optimizer = optim.SGD(model.parameters(), lr=0.01)


# COMMAND ----------

# MAGIC %md
# MAGIC ğŸ” Step 5: Training Loop
# MAGIC
# MAGIC Weâ€™ll train for 1000 epochs:
# MAGIC
# MAGIC Each time this loop runs, it does one epoch of training, which includes:
# MAGIC
# MAGIC âœ… 1. outputs = model(X)
# MAGIC This does a forward pass using your current model weights.
# MAGIC
# MAGIC ğŸ” Example (initial epoch):
# MAGIC
# MAGIC Your model starts with random weights and bias, say w1, w2, and b.
# MAGIC
# MAGIC It computes:
# MAGIC
# MAGIC ğ‘¦
# MAGIC ^
# MAGIC =
# MAGIC ğ‘‹
# MAGIC @
# MAGIC [
# MAGIC ğ‘¤
# MAGIC 1
# MAGIC ,
# MAGIC ğ‘¤
# MAGIC 2
# MAGIC ]
# MAGIC ğ‘‡
# MAGIC +
# MAGIC ğ‘
# MAGIC y
# MAGIC ^
# MAGIC â€‹
# MAGIC  =X@[w1,w2] 
# MAGIC T
# MAGIC  +b
# MAGIC This gives a prediction outputs for all 4 training samples.
# MAGIC
# MAGIC ğŸ“‰ 2. loss = criterion(outputs, y)
# MAGIC This computes the loss between predicted and actual values.
# MAGIC
# MAGIC Since we use Mean Squared Error (MSE):
# MAGIC
# MAGIC loss =
# MAGIC 1
# MAGIC ğ‘
# MAGIC âˆ‘
# MAGIC ğ‘–
# MAGIC =
# MAGIC 1
# MAGIC ğ‘
# MAGIC (
# MAGIC ğ‘¦
# MAGIC ^
# MAGIC ğ‘–
# MAGIC âˆ’
# MAGIC ğ‘¦
# MAGIC ğ‘–
# MAGIC )
# MAGIC 2
# MAGIC loss= 
# MAGIC N
# MAGIC 1
# MAGIC â€‹
# MAGIC   
# MAGIC i=1
# MAGIC âˆ‘
# MAGIC N
# MAGIC â€‹
# MAGIC  ( 
# MAGIC y
# MAGIC ^
# MAGIC â€‹
# MAGIC   
# MAGIC i
# MAGIC â€‹
# MAGIC  âˆ’y 
# MAGIC i
# MAGIC â€‹
# MAGIC  ) 
# MAGIC 2
# MAGIC  
# MAGIC So this tells us how wrong the model is right now.
# MAGIC
# MAGIC ğŸ”„ 3. optimizer.zero_grad()
# MAGIC This clears previous gradients from the last epoch.
# MAGIC If you skip this, gradients would accumulate (which we don't want in this case).
# MAGIC
# MAGIC ğŸ”™ 4. loss.backward()
# MAGIC This is the backpropagation step.
# MAGIC
# MAGIC It computes the gradient of the loss with respect to all model parameters (i.e., w1, w2, and b in our case).
# MAGIC
# MAGIC PyTorch tracks all operations during the forward pass and builds a computation graph, so when we call .backward(), it can automatically compute the gradients using the chain rule.
# MAGIC
# MAGIC ğŸ§  5. optimizer.step()
# MAGIC This is where the actual learning happens.
# MAGIC
# MAGIC It takes the gradients computed in the .backward() step and updates the weights in the direction that reduces the loss.
# MAGIC
# MAGIC In the case of SGD:
# MAGIC
# MAGIC ğ‘¤
# MAGIC :
# MAGIC =
# MAGIC ğ‘¤
# MAGIC âˆ’
# MAGIC lr
# MAGIC Ã—
# MAGIC âˆ‚
# MAGIC loss
# MAGIC âˆ‚
# MAGIC ğ‘¤
# MAGIC w:=wâˆ’lrÃ— 
# MAGIC âˆ‚w
# MAGIC âˆ‚loss
# MAGIC â€‹
# MAGIC  
# MAGIC where lr is the learning rate (0.01 in this case).
# MAGIC
# MAGIC ğŸ–¨ï¸ 6. Print Loss Every 100 Epochs
# MAGIC This helps you observe the loss reducing over time, like:

# COMMAND ----------

for epoch in range(1000):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)

    # Backward pass and optimization
    optimizer.zero_grad()  # Clear gradients
    loss.backward()        # Compute gradients
    optimizer.step()       # Update weights

    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}')


# COMMAND ----------

# MAGIC %md
# MAGIC âœ… Step 6: Test the Trained Model
# MAGIC

# COMMAND ----------

test_input = torch.tensor([[5.0, 6.0]])
predicted = model(test_input)
print("Prediction for [5,6]:", predicted.item())


# COMMAND ----------

print("Weights:", model.fc1.weight)
print("Bias:", model.fc1.bias)